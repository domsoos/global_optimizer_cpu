---
title: "Unconstrained Local Optimization: A Comparative Report of Quasi-Newton methods using the Multidimensional Rosenbrock function"
author: "Dominik So√≥s"
format:
  html:
    mainfont: Georgia
execute:
  echo: false
  warning: true
editor_options:
  markdown:
    wrap: sentence
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(readr)
library(ggplot2)
library(dplyr)
```
# Introduction
This report compares four different methods for solving optimization problems. We use a commonly used optimization function, Multidimensional Rosenbrock function, to test these algorithms. The methods we're looking at are DFP, BFGS, L-BFGS, and L-BFGS-B. We test them in different dimensions (10D, 20D, 50D, and 100D) to see how fast they run, how accurate they are, and how much computer memory they use. Our goal is to find out which method is the best for different situations. We expect L-BFGS and its variant L-BFGS-B to outperform the other two as we increase the number of dimensions. 

# Unconstrained Optimization Algorithm
-   **Require:**
    -   Objective function $f: \mathbb{R}^n \rightarrow \mathbb{R}$
    -   Gradient of the function $\nabla f: \mathbb{R}^n \rightarrow \mathbb{R}^n$
    -   Initial guess $x_0$
-   **Ensure:** Local minimum $x^*$ of $f$

1.  Set $k = 0$
2.  Choose an initial approximation $H_0$ to the inverse Hessian - the identity matrix
3.  While not converged:
    1.  Compute the search direction: $p_k = -H_k \nabla f(x_k)$
    2.  Calculate step size $\alpha_k$ using Line Search
    3.  Update the new point: $x_{k+1} = x_k + \alpha_k p_k$
    4.  Compute $\delta x = x_{k+1} - x_k$
    5.  Compute $\delta g = \nabla f(x_{k+1}) - \nabla f(x_k)$
    6.  Update formula for $H_{k+1}$
    7.  $k = k + 1$
4.  Return $x_k$ as the local minimum

# Davidon--Fletcher--Powell update
The primary goal of DFP is to update the approximation of the inverse of the Hessian matrix.
The update formula is: $$ H_{k+1} = H_k + \frac{\delta x \delta x^T}{\delta x^T \delta g} - \frac{H_k \delta g \delta g^T H_k}{\delta g^T H_k \delta g} $$

positive rank-1 update: $$\frac{\delta x \delta x^T}{\delta x^T \delta g} $$

negative rank-2 update: $$\frac{H_k \delta g \delta g^T H_k}{\delta g^T H_k \delta g} $$

# Broyden--Fletcher--Goldfarb--Shanno update
The BFGS method is another Quasi-Newton method that approximates the inverse of the Hessian using a combination of rank-1 updates.
The formula is: $$ H_{k+1} = \left(I - \frac{\delta x \delta g^T}{\delta x^T \delta g} \right) H_k \left(I - \frac{\delta g \delta x^T}{\delta x^T \delta g} \right) + \frac{\delta x \delta x^T}{\delta x^T \delta g} $$

which can be expanded: $$ H_{k+1} = H_k - H_k \frac{\delta x \delta g^T}{\delta x^T \delta g} H_k - \frac{\delta g \delta x^T}{\delta x^T \delta g} H_k + \frac{\delta x \delta x^T}{\delta x^T \delta g} $$

# Limited-memory BFGS or L-BFGS
L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) is an optimization algorithm for unconstrained optimization problems.
It is a member of the quasi-Newton family and is particularly well-suited for high-dimensional machine learning tasks aimed at minimizing a differentiable scalar function $f(\mathbf{x})$

Unlike the full BFGS algorithm, which requires storing a dense $n \times n$ inverse Hessian matrix, L-BFGS is memory-efficient.
It only stores a limited history of $m$ updates of the gradient $\nabla f(x)$ and position $x$, generally with $m < 10$.

The algorithm starts with an initial estimate $\mathbf{x}_0$ and iteratively refines this using estimates $\mathbf{x}_1, \mathbf{x}_2, \ldots$ The gradient $g_k = \nabla f(\mathbf{x}_k)$ is used to approximate the inverse Hessian and identify the direction of steepest descent.
The L-BFGS update for the inverse Hessian $H_{k+1}$ is given by:

$$
H_{k+1} = (I - \rho_k s_k y_k^\top) H_k (I - \rho_k y_k s_k^\top) + \rho_k s_k s_k^\top
$$

where $\rho_k = \frac{1}{y_k^\top s_k}$, $s_k = x_{k+1} - x_k$, $y_k = g_{k+1} - g_k$

## L-BFGS-B (L-BFGS with Box Constraints)

L-BFGS-B is an extension of L-BFGS that can handle bound constraints, i.e., constraints of the form 
$$
l \leq x \leq u
$$
The algorithm uses projected gradients to ensure that the bounds are obeyed at every iteration.


## MultiDimensional Rosenbrock Function
The Rosenbrock function is a commonly used test problem for optimization algorithms due to its non-convex nature and the difficulty in converging to the global minimum. 

10 Dimensional Rosenbrock Summation

$$
f(\mathbf{x}) = \sum_{i=0}^{9} \left[ (1 - x_i)^2 + 100 \cdot (x_{i+1} - x_i^2)^2 \right]
$$
where $\mathbf{x} = (x_1, x_2, \ldots, x_{10})$ and $\mathbf{x} \in \mathbb{R}^{10}$

We expect to see a difference in the amount of resources used by each algorithm as we increase the number of dimensions. The limited memory variant of BFGS, L-BFGS stores only a few vectors that implicitly represent the approximation of the inverse Hessian matrix. It should be highly suitable for high-dimensional problems.

### 10 Dimensions
#### Time in milliseconds
```{r}
data <- read.csv("data/memory_rosenbrock10.csv")

ggplot(data, aes(x=Time, color=Algorithm)) +xlim(c(0.0, 2000.0)) +  geom_density(alpha=0.5) + ggtitle("Density Plot of Time Across Algorithms")

ggplot(data, aes(x=Algorithm, y=Time)) + geom_boxplot() + ggtitle("Distribution of Time Across Algorithms")
```

### Error
Since we use local minimization, each algorithm gets stuck at the same point.  
```{r}
ggplot(data, aes(x=Algorithm, y=Error, color=Algorithm)) +
  geom_point(size=4) +
  ylim(min(data$Error) - 1e-10, max(data$Error) + 1e-10) +
  ggtitle("Scatter Plot of Error Across Algorithms")
# xlim(c(-10.0, 10.0))
summary_stats <- data %>%
  group_by(Algorithm) %>%
  summarise(
    mean = mean(Error),
    median = median(Error),
    sd = sd(Error),
    min = min(Error),
    max = max(Error)
  )
print(summary_stats,pillar = FALSE)
# Fit for Error
# Create scatter plot with conditional log-transformation for 'Error'
#ggplot(data, aes(x=Algorithm, y=ifelse(Error == 0, 0, log(Error)), color=Algorithm)) +
#  geom_point(size=4) +
# ggtitle("Conditionally Log-Transformed Scatter Plot of Error Across Algorithms")
```

#### Memory Usage
```{r}
# Plot for 'MemoryMB'
ggplot(data, aes(x=MemoryMB, color=Algorithm)) +xlim(c(0.0, 300.0)) +  geom_density(alpha=0.5) + ggtitle("Density Plot of MemoryMB Across Algorithms")
summary_stats <- data %>%
  group_by(Algorithm) %>%
  summarise(
    mean = mean(MemoryMB),
    median = median(MemoryMB),
    sd = sd(MemoryMB),
    min = min(MemoryMB),
    max = max(MemoryMB)
  )
print(summary_stats,pillar = FALSE)
```

### 20 dimensions
#### Time in seconds
```{r}
data <- read.csv("data/memory_rosenbrock20.csv")
data$Time <- data$Time / 60

ggplot(data, aes(x=Time, color=Algorithm)) + xlim(c(0.0, 30.0)) +  geom_density(alpha=0.5) + ggtitle("Density Plot of Time Across Algorithms")

ggplot(data, aes(x=Algorithm, y=Time)) + geom_boxplot() + ggtitle("Distribution of Time Across Algorithms")
```

#### Error
Same as before...
```{r}
ggplot(data, aes(x=Algorithm, y=Error, color=Algorithm)) +
  geom_point(size=4) +
  ylim(min(data$Error) - 1e-10, max(data$Error) + 1e-10) +
  ggtitle("Scatter Plot of Error Across Algorithms")

summary_stats <- data %>%
  group_by(Algorithm) %>%
  summarise(
    mean = mean(Error),
    median = median(Error),
    sd = sd(Error),
    min = min(Error),
    max = max(Error)
  )
print(summary_stats,pillar = FALSE)
# Fit for Error
# Create scatter plot with conditional log-transformation for 'Error'
#ggplot(data, aes(x=Algorithm, y=ifelse(Error == 0, 0, log(Error)), color=Algorithm)) +
#  geom_point(size=4) +
# ggtitle("Conditionally Log-Transformed Scatter Plot of Error Across Algorithms")
```

#### Memory Usage

```{r}
# Plot for 'MemoryMB'
ggplot(data, aes(x=MemoryMB, color=Algorithm)) + xlim(c(0.0, 400.0)) + geom_density(alpha=0.5) + ggtitle("Density Plot of MemoryMB Across Algorithms")
summary_stats <- data %>%
  group_by(Algorithm) %>%
  summarise(
    mean = mean(MemoryMB),
    median = median(MemoryMB),
    sd = sd(MemoryMB),
    min = min(MemoryMB),
    max = max(MemoryMB)
  )
print(summary_stats,pillar = FALSE)
```

### 50 dimensions
#### Time in seconds
```{r}
data <- read.csv("data/memory_rosenbrock50.csv")
data$Time <- data$Time / 60

ggplot(data, aes(x=Time, color=Algorithm)) +  geom_density(alpha=0.5) + ggtitle("Density Plot of Time Across Algorithms")

ggplot(data, aes(x=Algorithm, y=Time)) + geom_boxplot() + ggtitle("Distribution of Time Across Algorithms")
```

#### Memory Usage
```{r}
# Plot for 'MemoryMB'
ggplot(data, aes(x=MemoryMB, color=Algorithm)) + xlim(c(0.0, 1200.0)) + geom_density(alpha=0.5) + ggtitle("Density Plot of MemoryKB Across Algorithms")
summary_stats <- data %>%
  group_by(Algorithm) %>%
  summarise(
    mean = mean(MemoryMB),
    median = median(MemoryMB),
    sd = sd(MemoryMB),
    min = min(MemoryMB),
    max = max(MemoryMB)
  )
print(summary_stats,pillar = FALSE)
```

### 100 dimensions
#### Time in seconds
```{r}
data <- read.csv("data/memory_rosenbrock100.csv")
data$Time <- data$Time / 60

ggplot(data, aes(x=Time, color=Algorithm)) +  geom_density(alpha=0.5) + ggtitle("Density Plot of Time Across Algorithms")

ggplot(data, aes(x=Algorithm, y=Time)) + geom_boxplot() + ggtitle("Distribution of Time Across Algorithms")
```

#### Memory Usage
```{r}
# Plot for 'MemoryMB'
ggplot(data, aes(x=MemoryMB, color=Algorithm)) + xlim(c(0.0, 2500.0)) + geom_density(alpha=0.5) + ggtitle("Density Plot of MemoryKB Across Algorithms")
summary_stats <- data %>%
  group_by(Algorithm) %>%
  summarise(
    mean = mean(MemoryMB),
    median = median(MemoryMB),
    sd = sd(MemoryMB),
    min = min(MemoryMB),
    max = max(MemoryMB)
  )
print(summary_stats,pillar = FALSE)
```


### All dimensions concatenated data
#### Time in seconds
```{r}
data <- read.csv("data/memory_rosenbrock_sum.csv")
data$Time <- data$Time / 60

ggplot(data, aes(x=Time, color=Algorithm)) +  geom_density(alpha=0.5) + ggtitle("Density Plot of Time Across Algorithms")

ggplot(data, aes(x=Algorithm, y=Time)) + geom_boxplot() + ggtitle("Distribution of Time Across Algorithms")
```

#### Memory Usage
```{r}
# Plot for 'MemoryMB'
ggplot(data, aes(x=MemoryMB, color=Algorithm)) + xlim(c(0.0, 2500.0)) + geom_density(alpha=0.5) + ggtitle("Density Plot of MemoryMB Across Algorithms")
summary_stats <- data %>%
  group_by(Algorithm) %>%
  summarise(
    mean = mean(MemoryMB),
    median = median(MemoryMB),
    sd = sd(MemoryMB),
    min = min(MemoryMB),
    max = max(MemoryMB)
  )
print(summary_stats,pillar = FALSE)
```

# Data Analysis
## Time
The time taken for the algorithms varies significantly as the dimensionality increases. In 10D, L-BFGS-B is the fastest, followed by DFP, then L-BFGS, with BFGS being the slowest. This trend generally holds for higher dimensions as well, with L-BFGS-B consistently outperforming the others.

## Error
Since the objective is local optimization, the approximation error remains constant across all algorithms and dimensions.

## Memory
Memory usage follows a unique pattern. BFGS and DFP exhibit similar binomial memory curves, across all dimensions. In contrast, L-BFGS requires significantly more memory in low dimensional optimization, while L-BFGS-B offers a balanced profile with less memory usage and faster execution times.
